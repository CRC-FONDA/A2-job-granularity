{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataset from a path\n",
    "def read_dataset_single(path):\n",
    "    #returns a panda object with just the 3 columns Node, y, x\n",
    "    df = pd.read_csv(path, sep='\\t')\n",
    "    df = df[['Node Name', 'Total-time', 'Data Size']]\n",
    "    return df\n",
    "\n",
    "# read dataset from a list of paths\n",
    "def read_dataset_all(name_list):\n",
    "    # returns a panda object with all datasets combined\n",
    "    df_result = pd.DataFrame(['Node Name', 'Total-time', 'Data Size'])\n",
    "    \n",
    "    for path in name_list:\n",
    "        df_specific_result = read_dataset_single(path)\n",
    "        df_result = pd.concat([df_result, df_specific_result])\n",
    "        \n",
    "    return df_result\n",
    "\n",
    "\n",
    "# build a cleaned dataset with a minimum number of nodes, determined by the threshold\n",
    "def clean_data(df, threshold):\n",
    "    \n",
    "    nodename_counts = df['Node Name'].value_counts()\n",
    "    valid_nodenames = nodename_counts[nodename_counts >= threshold].index.tolist()\n",
    "    \n",
    "    return df[df['Nodename'].isin(valid_nodenames)]\n",
    "\n",
    "\n",
    "# returns array with all the unique nodenames\n",
    "def give_nodenames(df):\n",
    "    return np.unique(df['Node Name'].values)\n",
    "\n",
    "\n",
    "# returns just the x and y values for a Node as np.arrays\n",
    "# they are returned as sorted (by x) and unique\n",
    "# sort values (for easier fitting and interpolation)\n",
    "def give_x_and_y_per_node(df_general_result, name):\n",
    "\n",
    "    df = df_general_result[df_general_result['Node Name'] == name]\n",
    "    x, y = df['Data Size'].values, df['Total-time'].values\n",
    "    \n",
    "    unique_x, unique_indices = np.unique(x, return_index=True)\n",
    "    unique_y = y[unique_indices]\n",
    "\n",
    "    sorted_indices = np.argsort(unique_x)\n",
    "    \n",
    "    return unique_x[sorted_indices], unique_y[sorted_indices]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_data_and_fitted_functions(x_values, y_values, function_to_plot, titel_name):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # plot x,y\n",
    "    ax.scatter(x_values, y_values, label=\"Data Points\", color='b', marker='o')\n",
    "\n",
    "    # create estimations for plotting the function\n",
    "    x_range = np.linspace(min(x_values), max(x_values), 100)\n",
    "    y_range = function_to_plot(x_range)\n",
    "\n",
    "    ax.plot(x_range, y_range, label=\"function for estimation\", color='r')\n",
    "\n",
    "    ax.set_xlabel(\"X Values\")\n",
    "    ax.set_ylabel(\"Y Values\")\n",
    "    ax.set_title(titel_name)\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessery Functions for interpolation and curvefitting (poly1..4 and log1..4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for estimation\n",
    "def polynominal_estimation(x, coefficients):\n",
    "    return np.polyval(coefficients, x)\n",
    "\n",
    "def log_estimation(x, coefficients):\n",
    "    return np.log(np.polyval(coefficients, x))\n",
    "\n",
    "\n",
    "# for curve fitting\n",
    "#--------polynominal----------\n",
    "def poly_1_curve(x, a, b):\n",
    "    return np.polyval([a,b], x)\n",
    "\n",
    "def poly_2_curve(x, a, b, c):\n",
    "    return np.polyval([a,b,c], x)\n",
    "\n",
    "def poly_3_curve(x, a, b, c, d):\n",
    "    return np.polyval([a,b,c,d], x)\n",
    "\n",
    "def poly_4_curve(x, a, b, c, d, e):\n",
    "    return np.polyval([a,b,c,d,e], x)\n",
    "\n",
    "#----------log----------\n",
    "def log_1_curve(x, a, b):\n",
    "    return np.log(np.polyval([a,b], x))\n",
    "\n",
    "def log_2_curve(x, a, b, c):\n",
    "    return np.log(np.polyval([a,b,c], x))\n",
    "\n",
    "def log_3_curve(x, a, b, c, d):\n",
    "    return np.log(np.polyval([a,b,c,d], x))\n",
    "\n",
    "def log_4_curve(x, a, b, c, d, e):\n",
    "    return np.log(np.polyval([a,b,c,d,e], x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "\n",
    "\n",
    "#----------Fitting (Standart)----------\n",
    "def polynomial_fit(x, y, degree):\n",
    "    return np.polyfit(x, y, degree)\n",
    "\n",
    "# basically same function, but for better understanding here\n",
    "def log_polynominal_fitting(x, y_exp, degree):\n",
    "    return np.polyfit(x, y_exp, degree)\n",
    "\n",
    "\n",
    "#----------Curve Fitting----------\n",
    "# for large datasets, u should give a rough guess to speed up the fitting\n",
    "# here we usually use the results from the previous interpolations\n",
    "def curve_fitting_coefficients(x_values, y_values, function, guess):\n",
    "    coeff, _ = curve_fit(function, x_values, y_values, p0=guess)\n",
    "    return coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spline Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.interpolate as sci \n",
    "# more complicated than before, but the old stuff was deprecated and mostly legacy code\n",
    "# here we can also give us the spline functions and how they look\n",
    "# I implemented the fitting and the interpolation\n",
    "\n",
    "\n",
    "#----------Interpolation----------\n",
    "# import UnivariateSpline\n",
    "# from scipy.interpolate import InterpolatedUnivariateSpline\n",
    "# scipy.interpolate import make_smoothing_spline\n",
    "# for smoothing\n",
    "def get_smoothing_approximisation(values):\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#----------Spline interpolation (Standart)----------\n",
    "# it will create a spline function object over all datapoints with degreee k\n",
    "def spline_interpolation(x_values, y_values, degree):\n",
    "    return sci.make_interp_spline(x_values, y_values, k=degree)\n",
    "\n",
    "#----------Spline with smoothing----------\n",
    "# create spline object with smoothing build in (less knots)\n",
    "def spline_fitting_with_smoothing(x_values, y_values, degree):\n",
    "    return sci.UnivariateSpline\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#----------Fitting----------\n",
    "\n",
    "#----------Smooth Spline without k----------\n",
    "def smooth_spline_fitting(x_values, y_values):\n",
    "    return sci.make_smoothing_spline(x_values, y_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation for Smoothing\n",
    "If the weights represent the inverse of the standard-deviation of y, then a good s value should be found in the range (m-sqrt(2*m),m+sqrt(2*m)) where m is the number of datapoints in x, y, and w. This means s = len(w) should be a good value if 1/w[i] is an estimate of the standard deviation of y[i].\n",
    "\n",
    "from https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.UnivariateSpline.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Necessary Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "path_finals = [Path(\"/Users/manuez42/Desktop/fonda/A2-job-granularity/MG-HIBF/general_results_fonda/general_results_viral500m.tsv\"),\n",
    "               Path(\"/Users/manuez42/Desktop/fonda/A2-job-granularity/MG-HIBF/general_results_fonda/general_results_archea1.4g.tsv\"),\n",
    "               Path(\"/Users/manuez42/Desktop/fonda/A2-job-granularity/MG-HIBF/general_results_fonda/general_results_bacteria30g.tsv\"),\n",
    "               Path(\"/Users/manuez42/Desktop/fonda/A2-job-granularity/MG-HIBF/general_results_fonda/general_results_bacteria58g.tsv\"),\n",
    "               Path(\"/Users/manuez42/Desktop/fonda/A2-job-granularity/MG-HIBF/general_results_fonda/general_results_bacteria88g.tsv\"),\n",
    "               Path(\"/Users/manuez42/Desktop/fonda/A2-job-granularity/MG-HIBF/general_results_fonda/general_results_bacteria125g.tsv\")]\n",
    "\n",
    "name_list = [\"viral500m\", \"archea1.4g\", \"bacteria30g\", \"bacteria58\", \"bacteria88\", \"bacteria125\"]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
